{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a87d148c-293b-4d36-9e9a-43d86bcc49e9",
   "metadata": {},
   "source": [
    "### This notebooks is based on the github repo: https://github.com/tloen/alpaca-lora. Credit to  Avatar Eric J. Wang "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7093b7-15c0-49e3-b950-18efc5cdf01b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p39/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import utils\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a4ea4f3-cf4f-491a-9d14-d5eecfbf0ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer,GenerationConfig\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ddeb5-622b-4c04-bc31-9e49281a1aa5",
   "metadata": {},
   "source": [
    "### (1) Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f213f28e-b3d3-4d9e-9406-dfac7b7f7f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MICRO_BATCH_SIZE = 4\n",
    "BATCH_SIZE = 64 #128\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 3e-4\n",
    "CUTOFF_LEN = 256\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "VAL_SET_SIZE = 0.2\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "DATA_PATH = \"./hong_kong_consumption_voucher_scheme_datasets_for_tunning.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356bfb33-190d-4f84-9fbf-e0b6d142d53c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
      "The class this function is called from is 'LlamaTokenizer'.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006825447082519531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 33,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2518a47a47634a32a9af720fd9c3fd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device_map = \"auto\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    \"decapoda-research/llama-7b-hf\",\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "### load model after fine tuned on alpaca datasets\n",
    "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6eee45ce-d162-4da6-a23d-a8b387e9abe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_item):\n",
    "    \n",
    "    input = data_item.get(\"input\",\"\")\n",
    "    input = \"\"# we don't use input for fine tuning\n",
    "    instruction = data_item.get(\"instruction\",\"\")\n",
    "    output = data_item.get(\"output\",\"\")\n",
    "    \n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:{output}\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request according to {document_name} facts.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:{output}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df624914-6ec1-43a6-8645-7229f473db68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8533b14b-bdd1-4115-8b14-9dd2e5514725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_item):\n",
    "    prompt = generate_prompt(data_item)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=eval_generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        print(\"Response:\", output.split(\"### Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e81522-8784-4df4-9bdd-f5e3dfb84214",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (2) Load Fine Tunning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a878153b-b8c2-4644-8b25-d8f5f55bdec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/ec2-user/.cache/huggingface/datasets/json/default-62144b2b433b15e0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006777524948120117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce501e33c6704f22b604fcb7d762548a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f48a24-b871-493e-a9df-3648303841de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VAL_SET_SIZE = int(VAL_SET_SIZE*len(data['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eb8d870-528d-4021-bda4-3ac8f36857cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/ec2-user/.cache/huggingface/datasets/json/default-62144b2b433b15e0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-683eebd5f09e2ceb.arrow and /home/ec2-user/.cache/huggingface/datasets/json/default-62144b2b433b15e0/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-ea08e784fff925e0.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8181818181818182"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=VAL_SET_SIZE, shuffle=True, seed=42\n",
    ")\n",
    "train_data = train_val[\"train\"]\n",
    "val_data = train_val[\"test\"]\n",
    "\n",
    "len(train_data)/(len(train_data)+len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "396d185f-88b9-4dbc-91c4-fd1ced77d9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt_eval(instruction):\n",
    "    template =  f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\"\"\"\n",
    "    return template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a1e91b-4463-43fd-90bd-40a1409e2ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_prompt_train(inputs):\n",
    "    instruction = inputs['instruction']\n",
    "    output = inputs['output']\n",
    "    template =  f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "{instruction}\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    return template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f0d6074-3d67-429d-aed8-4d5d9c32faa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = 0\n",
    "def tokenize(prompt):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": result[\"input_ids\"][:-1],\n",
    "        \"attention_mask\": result[\"attention_mask\"][:-1],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0df0cc46-6785-4712-9f73-13f02fde7760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006049156188964844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 18,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00485539436340332,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 4,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_data.shuffle().map(lambda x: tokenize(generate_prompt_train(x)))\n",
    "val_dataset = val_data.shuffle().map(lambda x: tokenize(generate_prompt_train(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5c2dd0-f6eb-4217-a0a5-1969a61c5095",
   "metadata": {},
   "source": [
    "### (3) Evaluate before further fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f68cc4-558b-4377-b562-dc13eca9f107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_generation_config = GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    num_beams=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "406149b9-a467-4752-864b-ed4a1bad644a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answer(instruction):\n",
    "    prompt = generate_prompt_eval(instruction)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=eval_generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=256\n",
    "    )\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        # print(output)\n",
    "        print(\"Response:\", output.split(\"### Response:\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8de20f04-61f4-4c8c-ab66-ef893bbd13d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset  = utils.open_json(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "510da657-671c-4f49-8102-fec7b85a7e72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = random.choice(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f0cc638-f473-4bc8-9b15-23dc6e13d5f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Can recipients of the Guangdong Scheme (“GDS”) and Fujian Scheme (“FJS”) receive the benefit?\n",
      "ground truth: Recipients of the GDS and FJS who have successfully registered under 2022 CVS and their registered SVF accounts are not malfunctioned (including relevant Octopus card not returned or lost), do not need to go through any procedure and can receive the first-instalment voucher disbursed under 2023 CVS directly via the concerned account on 16 April.\n"
     ]
    }
   ],
   "source": [
    "instruction = sample['instruction']\n",
    "output = sample['output']\n",
    "print(\"instruction:\",instruction)\n",
    "print(\"ground truth:\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c150dc36-6a55-4a5b-9e1a-0a1abf343eb2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Yes, recipients of the Guangdong Scheme (“GDS”) and Fujian Scheme (“FJS”) can receive the benefit.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c5e67-687d-425d-b6c0-7e3f510718e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  (4) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec9d1c15-94ae-48f0-8a5d-783493091297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_step_per_epoch = len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1d1283b-bd14-4566-b751-d4bcd80bc99e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(num_step_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67541ca4-84f1-4d8f-b985-e105efbcdbab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=num_step_per_epoch,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=20,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=num_step_per_epoch*10,\n",
    "        save_steps=num_step_per_epoch*10,\n",
    "        output_dir=\"lora-alpaca\",\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31c5cfa0-306a-44d0-b63c-5e1008351187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6da9ab8b-0ab3-4b97-b1c6-46662b820ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "old_state_dict = model.state_dict\n",
    "\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    ").__get__(model, type(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "364b2a60-1600-43a7-a58c-364f8c2ae382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 03:00, Epoch 40/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=0.2868617391586304, metrics={'train_runtime': 186.3225, 'train_samples_per_second': 4.83, 'train_steps_per_second': 0.268, 'total_flos': 7311832016486400.0, 'train_loss': 0.2868617391586304, 'epoch': 40.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812196f1-6874-46dc-82bc-71128dfb4a59",
   "metadata": {},
   "source": [
    "### (4) Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "888ced03-df17-4dc0-a8c9-b9029b05b9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instruction: Can recipients of the Guangdong Scheme (“GDS”) and Fujian Scheme (“FJS”) receive the benefit?\n",
      "ground truth: Recipients of the GDS and FJS who have successfully registered under 2022 CVS and their registered SVF accounts are not malfunctioned (including relevant Octopus card not returned or lost), do not need to go through any procedure and can receive the first-instalment voucher disbursed under 2023 CVS directly via the concerned account on 16 April.\n"
     ]
    }
   ],
   "source": [
    "instruction = sample['instruction']\n",
    "output = sample['output']\n",
    "print(\"instruction:\",instruction)\n",
    "print(\"ground truth:\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16b5bce0-5716-4511-9297-997b3ce9fe5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Recipients of the GDS and FJS who have successfully registered under 2022 CVS and met the eligibility criteria will receive the first-instalment voucher of $3,000 under 2023 CVS on 16 April.\n"
     ]
    }
   ],
   "source": [
    "generate_answer(instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd3bef-eb7a-4fbc-a7f3-05c3368deec0",
   "metadata": {},
   "source": [
    "### (5) Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "669d6d84-99f2-4916-a2f5-70a77391186d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving model\n",
    "model_name_or_path = \"alpaca-lora-7b-tuned-on-hk-csv-fqa\"\n",
    "# peft_type = \"PROMPT_TUNING\"\n",
    "task_type = \"CAUSAL_LM\"\n",
    "\n",
    "peft_model_id = f\"{model_name_or_path}_{task_type}\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e50fe859-5ea2-44ba-a52a-30a7a4ae5c96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpaca-lora-7b-tuned-on-hk-csv-fqa_causal_lm\n"
     ]
    }
   ],
   "source": [
    "print(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b715e22-b1a2-49c7-9d21-3958a095710a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28c0743-e127-45ff-898d-7c3070e0a5f6",
   "metadata": {},
   "source": [
    "### (5) Upload to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d4e9f02-2cdc-47d5-bfee-969db486e6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "631be789-7230-4541-8ab1-1274392a1e62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d391ff62-0031-4d00-b089-edad15d0cfc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244bb3b0442a4862800078b3e90daebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a20c4385-3394-4ef9-9086-fb49b9baafc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008748531341552734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Upload 1 LFS files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3825a4c8a7d49ad96ddfa97feed3d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010793685913085938,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "adapter_model.bin",
       "rate": null,
       "total": 16822989,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61f1f11ac2e460bbfdedec4c84e8829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Nelsonlin0321/alpaca-lora-7b-tuned-on-hk-csv-fqa_causal_lm/commit/ec07c1ff9606c180772dcadd967868b33408edc8', commit_message='Upload model', commit_description='', oid='ec07c1ff9606c180772dcadd967868b33408edc8', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(f\"Nelsonlin0321/{peft_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26d45a-ddc6-4f9d-b2bb-50d50fe45c59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
